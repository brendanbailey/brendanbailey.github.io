{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks: In Class Codealong with MNIST Classfication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is based on the example given by Keras creater Francois Chollet [here](https://github.com/fchollet/keras/blob/master/examples/mnist_cnn.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this runthrough, we're going to be working with the MNIST dataset, which contains images of 70,000 handwritten digits. The dataset is a bit like Iris or Boston Housing, it's one of the core datasets for learning artificial neural networks. Convolutional Neural Networks have proven to be the most effective method of tackling image processing tasks, so we're going to work through classifying these handwritten digits with a CNN in Keras. \n",
    "\n",
    "**A bit on Keras:** Keras is an API that runs on top of the machine learning libraries Theano and Tensorflow. For context, it's a bit like sklearn for neural networks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist ##For loading the dataset\n",
    "from keras.utils.np_utils import to_categorical \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we can download the data with Keras. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "n_train, height, width = X_train.shape\n",
    "n_test, _, _ = X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets contains 60,000 28x28 training grayscale images and 10,000 28x28 test grayscale images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "We need to conduct just a few preprocessing steps to get the data into the format that we like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(n_train, 1, height, width).astype('float32')\n",
    "X_test = X_test.reshape(n_test, 1, height, width).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train, n_classes)\n",
    "y_test = to_categorical(y_test, n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture of CNNs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall, the general architecture of a convolutional neural network is: \n",
    "- convolution layers, followed by pooling layers\n",
    "- fully-connected layers\n",
    "- a final fully-connected softmax layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras gives us potentially one of the easiest ways to define an artificial neural network. To get started, we have to first initiate a **sequential model in Keras**, meaning that components and layers come one after another. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Convolution Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General thoughts for constructing the convolution layer:\n",
    "- The more complex the task, the more convolution layers we want in our network\n",
    "- We don't want our window to be too large, or the end matrix might not be that useful! \n",
    "- How large do we want our pooling to be? Approximately proporational to the size of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# number of convolutional windows\n",
    "n_filters = 32\n",
    "\n",
    "# convolution window size\n",
    "# i.e. we will use a n_conv x n_conv window\n",
    "n_conv = 3\n",
    "\n",
    "# pooling window size\n",
    "# i.e. we will use a n_pool x n_pool pooling window\n",
    "n_pool = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've set up these hyperparameters, we can begin adding layers to our network. We’re using only two convolutional layers because this is a relatively simple task. Generally for more complex tasks you may want more convolution layers to extract higher and higher level features.\n",
    "\n",
    "We're going to be using ReLu as our activation function. \n",
    "\n",
    "The particular pooling layer we’re using is a max pooling layer, which can be thought of as a “feature detector”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brendanbailey/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:13: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(data_format=\"channels_first\", input_shape=(1, 28, 28..., padding=\"valid\", filters=32, kernel_size=(3, 3))`\n",
      "/Users/brendanbailey/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:17: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3))`\n"
     ]
    }
   ],
   "source": [
    "model.add(Convolution2D(\n",
    "        filters = n_filters, kernel_size = (n_conv, n_conv),\n",
    "\n",
    "        # apply the window to only full parts of the image\n",
    "        # (i.e. do not \"spill over\" the border)\n",
    "        # this is called a narrow convolution\n",
    "        border_mode='valid',\n",
    "        \n",
    "        data_format = 'channels_first',\n",
    "\n",
    "        # we have a 28x28 single channel (grayscale) image\n",
    "        # so the input shape should be (1, 28, 28)\n",
    "        input_shape=(1, height, width)\n",
    "))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Convolution2D(n_filters, n_conv, n_conv))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# then we apply pooling to summarize the features\n",
    "# extracted thus far\n",
    "model.add(MaxPooling2D(pool_size=(n_pool, n_pool)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout + the Softmax Output Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recall Dropout**:\n",
    "- Dropout is a form of regularization for a neural network\n",
    "- It essentially forces an artificial neural network to learn multiple independent representations of the same data by alternately randomly disabling neurons in the learning phase.\n",
    "- The effect of this is that neurons are prevented from co-adapting too much which makes overfitting less likely.\n",
    "\n",
    "In Keras terminology, the dense layer is simply a **regular fully connected layer** for a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.add(Dropout(0.25))\n",
    "\n",
    "# flatten the data for the 1D layers (These are the output layers)\n",
    "model.add(Flatten())\n",
    "\n",
    "# Dense Layer(n_outputs)\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu')) ##Activation function for the dense layer. \n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# the softmax output layer gives us a probablity for each class\n",
    "model.add(Dense(n_classes))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling, Loss, and Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tell Keras to compile the model using whatever backend we have configured. At this stage we specify the loss function we want to optimize. Here we’re using categorical cross-entropy, which is the standard loss function for multiclass classification.\n",
    "\n",
    "We also specify the particular **optimization method** we want to use. An optimizer is one of the two arguments required for compiling a Keras model. You can either instantiate an optimizer before passing it to model.compile() , as in the above example, or you can call it by its name.  We've talked about plain vanilla Stochastic Gradient Descent (which we could use as an optimizer with SGD), however there are also varients which have been developed in the past few years that seek to perform further meta-optimization. One of these is Adam, developed in 2014. We're going to be using it here as it is the most recently developed iteration of stochastic gradient descent meta-optimization. You can read more about it [here](http://sebastianruder.com/optimizing-gradient-descent/index.html#adam). Adam adapts the learning rate based on how training is going and improves the training process.\n",
    "\n",
    "The second required arguement is the **loss function**. Here we’re using categorical cross-entropy, which is the standard loss function for multiclass classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'] \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've setup and compiled our network, we can begin training it! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 251s - loss: 0.3619 - acc: 0.8885 - val_loss: 0.0958 - val_acc: 0.9689\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 269s - loss: 0.1389 - acc: 0.9589 - val_loss: 0.0644 - val_acc: 0.9785\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 291s - loss: 0.1005 - acc: 0.9713 - val_loss: 0.0448 - val_acc: 0.9843\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 250s - loss: 0.0820 - acc: 0.9763 - val_loss: 0.0387 - val_acc: 0.9878\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 258s - loss: 0.0685 - acc: 0.9796 - val_loss: 0.0377 - val_acc: 0.9872\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 259s - loss: 0.0620 - acc: 0.9813 - val_loss: 0.0318 - val_acc: 0.9885\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 247s - loss: 0.0537 - acc: 0.9832 - val_loss: 0.0299 - val_acc: 0.9893\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 241s - loss: 0.0512 - acc: 0.9841 - val_loss: 0.0306 - val_acc: 0.9893\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 245s - loss: 0.0440 - acc: 0.9864 - val_loss: 0.0326 - val_acc: 0.9889\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 247s - loss: 0.0420 - acc: 0.9864 - val_loss: 0.0293 - val_acc: 0.9900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1332b1c50>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many examples to look at during each training iteration\n",
    "batch_size = 128\n",
    "\n",
    "# how many times to run through the full set of examples\n",
    "n_epochs = 10\n",
    "\n",
    "# the training may be slow depending on your computer\n",
    "model.fit(X_train,\n",
    "          y_train,\n",
    "          batch_size=batch_size,\n",
    "          nb_epoch=n_epochs,\n",
    "          validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then evaluate the model much like we would in sklearn: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9984/10000 [============================>.] - ETA: 0s('loss:', 0.029258942464456778)\n",
      "('accuracy:', 0.98999999999999999)\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print('loss:', loss)\n",
    "print('accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at that. That's a **99.1% classification accuracy** for unstructed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
